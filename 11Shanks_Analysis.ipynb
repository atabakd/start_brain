{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import keras\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import cPickle\n",
    "import h5py\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"11S_LP_DS.h5\", \"r\") as hf:\n",
    "  LFP_features_train = hf[\"LFP_features_train\"][...]\n",
    "  targets_train = hf[\"targets_train\"][...]\n",
    "  speeds_train = hf[\"speeds_train\"][...]\n",
    "  LFP_features_eval = hf[\"LFP_features_eval\"][...]\n",
    "  targets_eval = hf[\"targets_eval\"][...]\n",
    "  speeds_eval = hf[\"speeds_eval\"][...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And make sure it looks ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_sample = np.random.randint(LFP_features_train.shape[0])\n",
    "for i in range(LFP_features_train.shape[-1]):\n",
    "  plt.figure(figsize=(20,7))\n",
    "  plt_data = LFP_features_eval[93600,:,i]\n",
    "  plt.plot(np.arange(-0.5, 0., 0.5/plt_data.shape[0]), plt_data)\n",
    "  plt.xlable(\"time\")\n",
    "  plt.title(str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write some helper functions to easily select regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "good_channels = np.load('good_channels.npy')\n",
    "front_idxs = np.where([good_channels<64])[0]\n",
    "back_idxs = np.where([good_channels>=64])[0]\n",
    "bottom_channels = [76,83,80,78,89,100,70,64,94,61,67,42,84,52,56,38,22,8,24,39,4,26,47,\n",
    "                           37,45,59,46,25,7]\n",
    "bottom_idxs = np.sort(np.stack([np.where(good_channels==bc)[0] for bc in bottom_channels]).squeeze())\n",
    "top_idxs = np.stack([i for i in range(good_channels.shape[0]) if i not in bottom_idxs]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LFP_features_train_current = LFP_features_train\n",
    "LFP_features_eval_current = LFP_features_eval\n",
    "# LFP_features_train_current = LFP_features_train[...,bottom_idxs]\n",
    "# LFP_features_eval_current = LFP_features_eval[...,bottom_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a call back to save the best validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_chk_path = 'my_model.hdf5'\n",
    "mcp = keras.callbacks.ModelCheckpoint(model_chk_path, monitor=\"val_acc\",\n",
    "                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I have defined a couple of different network architectures to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#   model = None\n",
    "# except NameError:\n",
    "#   pass\n",
    "# decay = 1e-3\n",
    "# conv1d = keras.layers.Convolution1D\n",
    "# maxPool = keras.layers.MaxPool1D\n",
    "# model = keras.models.Sequential()\n",
    "# model.add(conv1d(64, 5, padding='same', strides=2, activation='relu', \n",
    "#                  kernel_regularizer=keras.regularizers.l2(decay),\n",
    "#                  input_shape=LFP_features_train_current.shape[1:]))\n",
    "# model.add(maxPool())\n",
    "# model.add(conv1d(128, 3, padding='same', strides=2, activation='relu', \n",
    "#                  kernel_regularizer=keras.regularizers.l2(decay)))\n",
    "# model.add(maxPool())\n",
    "# model.add(conv1d(128, 3, padding='same', strides=2, activation='relu', \n",
    "#                  kernel_regularizer=keras.regularizers.l2(decay)))\n",
    "# model.add(maxPool())\n",
    "# model.add(conv1d(128, 3, padding='same', strides=2, activation='relu', \n",
    "#                  kernel_regularizer=keras.regularizers.l2(decay)))\n",
    "# model.add(maxPool())\n",
    "# model.add(keras.layers.Flatten())\n",
    "# model.add(keras.layers.Dropout(rate=0.5))\n",
    "# model.add(keras.layers.Dense(2, activation='softmax', kernel_regularizer=keras.regularizers.l2(decay)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#   model = None\n",
    "# except NameError:\n",
    "#   pass\n",
    "# decay = 1e-3\n",
    "# conv1d = keras.layers.Convolution1D\n",
    "# maxPool = keras.layers.MaxPool1D\n",
    "# model = keras.models.Sequential()\n",
    "# model.add(conv1d(33, 5, padding='same', activation='relu', kernel_regularizer=keras.regularizers.l2(decay),\n",
    "#                  input_shape=LFP_features_train.shape[1:]))\n",
    "# model.add(maxPool())\n",
    "# model.add(conv1d(33, 3, padding='same', activation='relu', kernel_regularizer=keras.regularizers.l2(decay)))\n",
    "# model.add(maxPool())\n",
    "# model.add(conv1d(16, 3, padding='same', activation='relu', kernel_regularizer=keras.regularizers.l2(decay)))\n",
    "# model.add(maxPool())\n",
    "# model.add(conv1d(4, 3, padding='same', activation='relu', kernel_regularizer=keras.regularizers.l2(decay)))\n",
    "# model.add(maxPool())\n",
    "# model.add(keras.layers.Flatten())\n",
    "# model.add(keras.layers.Dropout(rate=0.5))\n",
    "# model.add(keras.layers.Dense(2, activation='softmax', kernel_regularizer=keras.regularizers.l2(decay)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  model = None\n",
    "except NameError:\n",
    "  pass\n",
    "decay = 1e-3\n",
    "regul = keras.regularizers.l1(decay)\n",
    "conv1d = keras.layers.Convolution1D\n",
    "maxPool = keras.layers.MaxPool1D\n",
    "BN = keras.layers.BatchNormalization\n",
    "Act = keras.layers.Activation('relu')\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Convolution1D(64, 5, padding='same', strides=2, \n",
    "                 kernel_regularizer=keras.regularizers.l1(decay),\n",
    "                 input_shape=LFP_features_train_current.shape[1:]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.MaxPool1D())\n",
    "model.add(keras.layers.Convolution1D(128, 3, padding='same', strides=2,\n",
    "                 kernel_regularizer=keras.regularizers.l1(decay)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.MaxPool1D())\n",
    "model.add(keras.layers.Convolution1D(128, 3, padding='same', strides=2,\n",
    "                 kernel_regularizer=keras.regularizers.l1(decay)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "\n",
    "# model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.MaxPool1D())\n",
    "model.add(keras.layers.Convolution1D(128, 3, padding='same', strides=2,\n",
    "                 kernel_regularizer=keras.regularizers.l1(decay)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "# model.add(maxPool())\n",
    "# model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dropout(rate=0.5))\n",
    "model.add(keras.layers.Dense(2, activation='softmax', kernel_regularizer=keras.regularizers.l1_l2(decay)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#   model = None\n",
    "# except NameError:\n",
    "#   pass\n",
    "# decay = 1e-3\n",
    "# conv1d = keras.layers.Convolution1D\n",
    "# maxPool = keras.layers.MaxPool1D\n",
    "# model = keras.models.Sequential()\n",
    "# model.add(conv1d(64, 5, padding='same', strides=2, activation='relu', \n",
    "#                  kernel_regularizer=keras.regularizers.l2(decay),\n",
    "#                  input_shape=LFP_features_train_current.shape[1:]))\n",
    "# model.add(maxPool())\n",
    "\n",
    "# model.add(conv1d(128, 3, padding='same', strides=1, activation='relu', \n",
    "#                  kernel_regularizer=keras.regularizers.l2(decay)))\n",
    "# model.add(maxPool())\n",
    "\n",
    "# model.add(keras.layers.LSTM(128))\n",
    "# # model.add(conv1d(128, 3, padding='same', strides=2, activation='relu', \n",
    "# #                  kernel_regularizer=keras.regularizers.l2(decay)))\n",
    "# # model.add(maxPool())\n",
    "# # model.add(conv1d(128, 3, padding='same', strides=2, activation='relu', \n",
    "# #                  kernel_regularizer=keras.regularizers.l2(decay)))\n",
    "# # model.add(maxPool())\n",
    "# # model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# model.add(keras.layers.Dense(2, activation='softmax', kernel_regularizer=keras.regularizers.l2(decay)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#   model = None\n",
    "# except NameError:\n",
    "#   pass\n",
    "# decay = 1e-3\n",
    "# regul = keras.regularizers.l1_l2(decay)\n",
    "# conv1d = keras.layers.Convolution1D\n",
    "# maxPool = keras.layers.MaxPool1D\n",
    "# BN = keras.layers.BatchNormalization\n",
    "# Act = keras.layers.Activation('relu')\n",
    "# model = keras.models.Sequential()\n",
    "# model.add(conv1d(64, 5, padding='same', strides=2, \n",
    "#                  kernel_regularizer=regul,\n",
    "#                  input_shape=LFP_features_train_current.shape[1:]))\n",
    "# model.add(BN())\n",
    "# model.add(Act)\n",
    "# model.add(maxPool())\n",
    "# model.add(conv1d(128, 3, padding='same', strides=2,\n",
    "#                  kernel_regularizer=regul))\n",
    "# model.add(BN())\n",
    "# model.add(Act)\n",
    "# model.add(maxPool())\n",
    "# model.add(keras.layers.LSTM(128, dropout=0.1, recurrent_dropout=0.5))\n",
    "# # model.add(conv1d(128, 3, padding='same', strides=2, activation='relu', \n",
    "# #                  kernel_regularizer=keras.regularizers.l2(decay)))\n",
    "# # model.add(maxPool())\n",
    "# # model.add(conv1d(128, 3, padding='same', strides=2, activation='relu', \n",
    "# #                  kernel_regularizer=keras.regularizers.l2(decay)))\n",
    "# # model.add(maxPool())\n",
    "# # model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# model.add(keras.layers.Dense(2, activation='softmax', kernel_regularizer=keras.regularizers.l2(decay)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(LFP_features_train_current,\n",
    "                    targets_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=1024,\n",
    "                    validation_data=(LFP_features_eval_current, targets_eval),\n",
    "                    callbacks=[mcp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "  \"\"\"\n",
    "  This function prints and plots the confusion matrix.\n",
    "  Normalization can be applied by setting `normalize=True`.\n",
    "  \"\"\"\n",
    "  if normalize:\n",
    "      cm = cm.astype('float') / np.maximum(cm.sum(axis=1)[:, np.newaxis],1.0)\n",
    "      print(\"Normalized confusion matrix\")\n",
    "  else:\n",
    "      print('Confusion matrix, without normalization')\n",
    "\n",
    "  print(cm)\n",
    "  \n",
    "  cm = (cm*1000).astype(np.int16)\n",
    "  cm = np.multiply(cm, 0.1)\n",
    "  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "  plt.title(title)\n",
    "  plt.colorbar()\n",
    "  tick_marks = np.arange(len(classes))\n",
    "  plt.xticks(tick_marks, classes, rotation=45)\n",
    "  plt.yticks(tick_marks, classes)\n",
    "  thresh = cm.max() / 2.\n",
    "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "      plt.text(j, i, \"{0}%\".format(cm[i, j]),\n",
    "               horizontalalignment=\"center\",\n",
    "               color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.ylabel('True label')\n",
    "  plt.xlabel('Predicted label')\n",
    "  return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_names = ['go', 'stop']\n",
    "model.load_weights('my_model_bottom.hdf5')\n",
    "y_pred_initial = model.predict(LFP_features_eval)\n",
    "targets_eval_1d = np.argmax(targets_eval, axis=1)\n",
    "y_pred = np.argmax(y_pred_initial, axis=1)\n",
    "cnf_matrix = confusion_matrix(targets_eval_1d, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "fig = plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrong_idxs = np.where(y_pred != targets_eval_1d)[0]\n",
    "wrong_vals = speeds_eval[wrong_idxs]\n",
    "# wrong_vals.squeeze().shape\n",
    "# crazy_wrong_idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.cla()\n",
    "plt.close()\n",
    "plt.figure(figsize=(20,7))\n",
    "n, bins, patches = plt.hist(wrong_vals.squeeze(), \n",
    "                            bins=np.arange(0,1,0.01),)\n",
    "\n",
    "plt.plot(bins)\n",
    "plt.xlim([0,1])\n",
    "fig_dist = plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluation accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "plt.figure(figsize=(20,7))\n",
    "plt.plot(epochs, acc, 'bo', label='Training')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(loc='lower right', fontsize=24)\n",
    "plt.xticks(np.arange(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we make sure the mean impedance of channels in different regions are not that different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "impedances = np.genfromtxt('/home/atabak/test_day/IMPEDANCE.csv', delimiter=',', usecols=4)[1:]\n",
    "bottom_channels = [76,83,80,78,89,100,70,64,94,61,67,42,84,52,56,38,22,8,24,39,4,26,47,\n",
    "                           37,45,59,46,25,7]\n",
    "top_channels = [i for i in good_channels if i not in bottom_channels]\n",
    "\n",
    "front_channels = good_channels[good_channels<64]\n",
    "back_channels = good_channels[good_channels>=64]\n",
    "print(\"mean of bottom good channels is: {0}\".format(impedances[bottom_channels].mean()))\n",
    "print(\"mean of top good channels is: {0}\".format(impedances[top_channels].mean()))\n",
    "print(\"mean of front good channels is: {0}\".format(impedances[front_channels].mean()))\n",
    "print(\"mean of back good channels is: {0}\".format(impedances[back_channels].mean()))\n",
    "print(\"mean of good channels is: {0}\".format(impedances[good_channels].mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
