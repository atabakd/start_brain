{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import keras\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import cPickle #python 2.x\n",
    "#import _pickle as cPickle #python 3.x\n",
    "import h5py\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"NS_LP_DS.h5\", \"r\") as hf:\n",
    "  LFP_features_train = hf[\"LFP_features_train\"][...]\n",
    "  targets_train = hf[\"targets_train\"][...]\n",
    "  speeds_train = hf[\"speeds_train\"][...]\n",
    "  LFP_features_eval = hf[\"LFP_features_eval\"][...]\n",
    "  targets_eval = hf[\"targets_eval\"][...]\n",
    "  speeds_eval = hf[\"speeds_eval\"][...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And make sure it looks ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_sample = np.random.randint(LFP_features_eval.shape[0])\n",
    "for i in range(LFP_features_train.shape[-1]):\n",
    "  plt.figure(figsize=(20,7))\n",
    "  plt_data = LFP_features_eval[rand_sample,:,i]\n",
    "  plt.plot(np.arange(-0.5, 0., 0.5/plt_data.shape[0]), plt_data)\n",
    "  plt.xlable(\"time\")\n",
    "  plt.title(str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write some helper functions to easily select regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "block = np.array([[2,4,6,8],[1,3,5,7]])\n",
    "channels = np.concatenate([(block + i*8) for i in range(180)][::-1])\n",
    "brain_regions = {'Parietal Cortex': 8000, 'Hypocampus CA1': 6230, 'Hypocampus DG': 5760, 'Thalamus LPMR': 4450,\n",
    "                 'Thalamus Posterior': 3500, 'Thalamus VPM': 1930, 'SubThalamic': 1050}\n",
    "brain_regions = {k:v//22.5 for k,v in brain_regions.iteritems()}\n",
    "used_channels = np.arange(9,1440,20, dtype=np.int16)[:-6]\n",
    "for i in (729,749,1209,1229):\n",
    "  used_channels = np.delete(used_channels, np.where(used_channels==i)[0])\n",
    "\n",
    "# for k,v in brain_regions.iteritems():\n",
    "#   print(\"{0}: {1}\".format(k,v))\n",
    "  \n",
    "channels_dict = {'Parietal Cortex': np.arange(1096,1440, dtype=np.int16), \n",
    "                 'Hypocampus CA1': np.arange(1016,1096, dtype=np.int16), \n",
    "                 'Hypocampus DG': np.arange(784,1016, dtype=np.int16), \n",
    "                 'Thalamus LPMR': np.arange(616,784, dtype=np.int16),\n",
    "                 'Thalamus Posterior': np.arange(340,616, dtype=np.int16), \n",
    "                 'Thalamus VPM': np.arange(184,340, dtype=np.int16), \n",
    "                 'SubThalamic': np.arange(184, dtype=np.int16)}\n",
    "used_channels_dict = {k:list() for k in channels_dict.iterkeys()}\n",
    "# print(\"hello\")\n",
    "for ch in used_channels:\n",
    "  for key in channels_dict.iterkeys():\n",
    "    if ch in channels_dict[key]:\n",
    "      used_channels_dict[key].append(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LFP_features_train_current = LFP_features_train\n",
    "LFP_features_eval_current = LFP_features_eval\n",
    "# current_channels = np.sort(used_channels_dict['Hypocampus CA1']+used_channels_dict['Hypocampus DG']+\\\n",
    "#                             used_channels_dict['Thalamus Posterior'])\n",
    "# current_idxs = np.array([np.where(ch==used_channels)[0] for ch in current_channels]).squeeze()\n",
    "# LFP_features_train_current = LFP_features_train[...,current_idxs]\n",
    "# LFP_features_eval_current = LFP_features_eval[...,current_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a call back to save the best validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_chk_path = 'my_model.hdf5'\n",
    "mcp = keras.callbacks.ModelCheckpoint(model_chk_path, monitor=\"val_acc\",\n",
    "                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I have defined a couple of different network architectures to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#   model = None\n",
    "# except NameError:\n",
    "#   pass\n",
    "# decay = 1e-3\n",
    "# conv1d = keras.layers.Convolution1D\n",
    "# maxPool = keras.layers.MaxPool1D\n",
    "# model = keras.models.Sequential()\n",
    "# model.add(conv1d(64, 5, padding='same', strides=2, activation='relu', \n",
    "#                  kernel_regularizer=keras.regularizers.l2(decay),\n",
    "#                  input_shape=LFP_features_train.shape[1:]))\n",
    "# model.add(maxPool())\n",
    "# model.add(conv1d(128, 3, padding='same', strides=2, activation='relu', \n",
    "#                  kernel_regularizer=keras.regularizers.l2(decay)))\n",
    "# model.add(maxPool())\n",
    "# model.add(conv1d(128, 3, padding='same', strides=2, activation='relu', \n",
    "#                  kernel_regularizer=keras.regularizers.l2(decay)))\n",
    "# model.add(maxPool())\n",
    "# model.add(conv1d(128, 3, padding='same', strides=2, activation='relu', \n",
    "#                  kernel_regularizer=keras.regularizers.l2(decay)))\n",
    "# model.add(maxPool())\n",
    "# model.add(keras.layers.Flatten())\n",
    "# model.add(keras.layers.Dropout(rate=0.5))\n",
    "# model.add(keras.layers.Dense(2, activation='softmax', kernel_regularizer=keras.regularizers.l2(decay)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#   model = None\n",
    "# except NameError:\n",
    "#   pass\n",
    "# decay = 1e-3\n",
    "# conv1d = keras.layers.Convolution1D\n",
    "# maxPool = keras.layers.MaxPool1D\n",
    "# BN = keras.layers.BatchNormalization\n",
    "# Act = keras.layers.Activation('relu')\n",
    "# model = keras.models.Sequential()\n",
    "# model.add(conv1d(64, 5, padding='same', strides=2, \n",
    "#                  kernel_regularizer=keras.regularizers.l1_l2(decay),\n",
    "#                  input_shape=LFP_features_train_current.shape[1:]))\n",
    "# model.add(BN())\n",
    "# model.add(Act)\n",
    "# model.add(maxPool())\n",
    "# model.add(conv1d(128, 3, padding='same', strides=2,\n",
    "#                  kernel_regularizer=keras.regularizers.l1_l2(decay)))\n",
    "# model.add(BN())\n",
    "# model.add(Act)\n",
    "# model.add(maxPool())\n",
    "# model.add(conv1d(128, 3, padding='same', strides=2,\n",
    "#                  kernel_regularizer=keras.regularizers.l1_l2(decay)))\n",
    "# model.add(BN())\n",
    "# model.add(Act)\n",
    "# model.add(maxPool())\n",
    "# model.add(conv1d(128, 3, padding='same', strides=2,\n",
    "#                  kernel_regularizer=keras.regularizers.l1_l2(decay)))\n",
    "# model.add(BN())\n",
    "# model.add(Act)\n",
    "# model.add(maxPool())\n",
    "# model.add(keras.layers.Flatten())\n",
    "# model.add(keras.layers.Dropout(rate=0.5))\n",
    "# model.add(keras.layers.Dense(2, activation='softmax', kernel_regularizer=keras.regularizers.l2(decay)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#   model = None\n",
    "# except NameError:\n",
    "#   pass\n",
    "# decay = 1e-3\n",
    "# conv1d = keras.layers.Convolution1D\n",
    "# maxPool = keras.layers.MaxPool1D\n",
    "# model = keras.models.Sequential()\n",
    "# model.add(conv1d(33, 5, padding='same', activation='relu', kernel_regularizer=keras.regularizers.l2(decay),\n",
    "#                  input_shape=LFP_features_train.shape[1:]))\n",
    "# model.add(maxPool())\n",
    "# model.add(conv1d(33, 3, padding='same', activation='relu', kernel_regularizer=keras.regularizers.l2(decay)))\n",
    "# model.add(maxPool())\n",
    "# model.add(conv1d(16, 3, padding='same', activation='relu', kernel_regularizer=keras.regularizers.l2(decay)))\n",
    "# model.add(maxPool())\n",
    "# model.add(conv1d(4, 3, padding='same', activation='relu', kernel_regularizer=keras.regularizers.l2(decay)))\n",
    "# model.add(maxPool())\n",
    "# model.add(keras.layers.Flatten())\n",
    "# model.add(keras.layers.Dropout(rate=0.5))\n",
    "# model.add(keras.layers.Dense(2, activation='softmax', kernel_regularizer=keras.regularizers.l2(decay)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  model = None\n",
    "except NameError:\n",
    "  pass\n",
    "decay = 1e-3\n",
    "regul = keras.regularizers.l1(decay)\n",
    "conv1d = keras.layers.Convolution1D\n",
    "maxPool = keras.layers.MaxPool1D\n",
    "BN = keras.layers.BatchNormalization\n",
    "Act = keras.layers.Activation('relu')\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Convolution1D(64, 5, padding='same', strides=2, \n",
    "                 kernel_regularizer=keras.regularizers.l1_l2(decay),\n",
    "                 input_shape=LFP_features_train_current.shape[1:]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.MaxPool1D())\n",
    "model.add(keras.layers.Convolution1D(128, 3, padding='same', strides=2,\n",
    "                 kernel_regularizer=keras.regularizers.l1_l2(decay)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "# model.add(keras.layers.MaxPool1D())\n",
    "# model.add(keras.layers.Convolution1D(128, 3, padding='same', strides=2,\n",
    "#                  kernel_regularizer=keras.regularizers.l1_l2(decay)))\n",
    "# model.add(keras.layers.BatchNormalization())\n",
    "# model.add(keras.layers.Activation('relu'))\n",
    "\n",
    "# # model.add(keras.layers.GlobalMaxPooling1D())\n",
    "# model.add(keras.layers.MaxPool1D())\n",
    "# model.add(keras.layers.Convolution1D(128, 3, padding='same', strides=2,\n",
    "#                  kernel_regularizer=keras.regularizers.l1_l2(decay)))\n",
    "# model.add(keras.layers.BatchNormalization())\n",
    "# model.add(keras.layers.Activation('relu'))\n",
    "# model.add(maxPool())\n",
    "# model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dropout(rate=0.5))\n",
    "model.add(keras.layers.Dense(2, activation='softmax', kernel_regularizer=keras.regularizers.l1_l2(decay)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(LFP_features_train_current,\n",
    "                    targets_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=1024,\n",
    "                    validation_data=(LFP_features_eval_current, targets_eval),\n",
    "                    callbacks=[mcp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "  \"\"\"\n",
    "  This function prints and plots the confusion matrix.\n",
    "  Normalization can be applied by setting `normalize=True`.\n",
    "  \"\"\"\n",
    "  if normalize:\n",
    "      cm = cm.astype('float') / np.maximum(cm.sum(axis=1)[:, np.newaxis],1.0)\n",
    "      print(\"Normalized confusion matrix\")\n",
    "  else:\n",
    "      print('Confusion matrix, without normalization')\n",
    "\n",
    "  print(cm)\n",
    "  \n",
    "  cm = (cm*1000).astype(np.int16)\n",
    "  cm = np.multiply(cm, 0.1)\n",
    "  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "  plt.title(title)\n",
    "  plt.colorbar()\n",
    "  tick_marks = np.arange(len(classes))\n",
    "  plt.xticks(tick_marks, classes, rotation=45)\n",
    "  plt.yticks(tick_marks, classes)\n",
    "  thresh = cm.max() / 2.\n",
    "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "      plt.text(j, i, \"{0}%\".format(cm[i, j]),\n",
    "               horizontalalignment=\"center\",\n",
    "               color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.ylabel('True label')\n",
    "  plt.xlabel('Predicted label')\n",
    "  return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_names = ['go', 'stop']\n",
    "model.load_weights('my_model.hdf5')\n",
    "y_pred_initial = model.predict(LFP_features_eval)\n",
    "targets_eval_1d = np.argmax(targets_eval, axis=1)\n",
    "y_pred = np.argmax(y_pred_initial, axis=1)\n",
    "cnf_matrix = confusion_matrix(targets_eval_1d, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "fig = plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrong_idxs = np.where(y_pred != targets_eval_1d)[0]\n",
    "wrong_vals = speeds_eval[wrong_idxs]\n",
    "# wrong_vals.squeeze().shape\n",
    "# crazy_wrong_idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.cla()\n",
    "plt.close()\n",
    "plt.figure(figsize=(20,7))\n",
    "n, bins, patches = plt.hist(wrong_vals.squeeze(), \n",
    "                            bins=np.arange(0,1,0.01),)\n",
    "\n",
    "plt.plot(bins)\n",
    "plt.xlim([0,1])\n",
    "fig_dist = plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluation accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "plt.figure(figsize=(20,7))\n",
    "plt.plot(epochs, acc, 'bo', label='Training')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(loc='lower right', fontsize=24)\n",
    "plt.xticks(np.arange(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
